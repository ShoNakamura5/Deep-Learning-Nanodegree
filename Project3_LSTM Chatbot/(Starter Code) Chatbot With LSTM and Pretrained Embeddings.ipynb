{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "・https://torch.classcat.com/2018/05/15/pytorch-tutorial-intermediate-seq2seq-translation/ \n",
    "\n",
    "・https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchtext==0.9.0\n",
      "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1 MB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtext==0.9.0) (4.43.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchtext==0.9.0) (2.23.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtext==0.9.0) (1.21.2)\n",
      "Collecting torch==1.8.0\n",
      "  Downloading torch-1.8.0-cp37-cp37m-manylinux1_x86_64.whl (735.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 735.5 MB 9.2 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.9.0) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.9.0) (1.25.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.9.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.9.0) (2019.11.28)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.8.0->torchtext==0.9.0) (3.7.4.1)\n",
      "\u001b[31mERROR: torchvision 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.8.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: torch, torchtext\n",
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx and convert-onnx-to-caffe2 are installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed torch-1.8.0 torchtext-0.9.0\n"
     ]
    }
   ],
   "source": [
    "# Restart after run\n",
    "!pip install torchtext==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import torch\n",
    "from nltk.corpus import brown\n",
    "from torchtext.datasets import SQuAD1\n",
    "import string\n",
    "import torch.nn as nn\n",
    "import random \n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer('english')\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Output, save, and load brown embeddings\n",
    "\n",
    "model = gensim.models.Word2Vec(brown.sents())\n",
    "model.save('brown.embedding')\n",
    "\n",
    "w2v = gensim.models.Word2Vec.load('brown.embedding')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load SQuAD1.0 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDF(path):\n",
    "    \n",
    "    dataset_train, dataset_dev = SQuAD1(root = path, split = ('train', 'dev'))\n",
    "\n",
    "    df_train = pd.DataFrame.from_dict(dataset_train)\n",
    "    df_dev = pd.DataFrame.from_dict(dataset_dev)\n",
    "    \n",
    "    df = df_train.append(df_dev)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>[Saint Bernadette Soubirous]</td>\n",
       "      <td>[515]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>[a copper statue of Christ]</td>\n",
       "      <td>[188]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>[the Main Building]</td>\n",
       "      <td>[279]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>[a Marian place of prayer and reflection]</td>\n",
       "      <td>[381]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>[a golden statue of the Virgin Mary]</td>\n",
       "      <td>[92]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0  Architecturally, the school has a Catholic cha...   \n",
       "1  Architecturally, the school has a Catholic cha...   \n",
       "2  Architecturally, the school has a Catholic cha...   \n",
       "3  Architecturally, the school has a Catholic cha...   \n",
       "4  Architecturally, the school has a Catholic cha...   \n",
       "\n",
       "                                            Question  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...   \n",
       "1  What is in front of the Notre Dame Main Building?   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...   \n",
       "3                  What is the Grotto at Notre Dame?   \n",
       "4  What sits on top of the Main Building at Notre...   \n",
       "\n",
       "                                      Answer      ?  \n",
       "0               [Saint Bernadette Soubirous]  [515]  \n",
       "1                [a copper statue of Christ]  [188]  \n",
       "2                        [the Main Building]  [279]  \n",
       "3  [a Marian place of prayer and reflection]  [381]  \n",
       "4       [a golden statue of the Virgin Mary]   [92]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = loadDF('.data')\n",
    "\n",
    "feature = [\"Sentence\", \"Question\", \"Answer\", \"?\"]\n",
    "df.columns = feature\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>[Saint Bernadette Soubirous]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>[a copper statue of Christ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>[the Main Building]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>[a Marian place of prayer and reflection]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>[a golden statue of the Virgin Mary]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...   \n",
       "1  What is in front of the Notre Dame Main Building?   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...   \n",
       "3                  What is the Grotto at Notre Dame?   \n",
       "4  What sits on top of the Main Building at Notre...   \n",
       "\n",
       "                                      Answer  \n",
       "0               [Saint Bernadette Soubirous]  \n",
       "1                [a copper statue of Christ]  \n",
       "2                        [the Main Building]  \n",
       "3  [a Marian place of prayer and reflection]  \n",
       "4       [a golden statue of the Virgin Mary]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"Question\", \"Answer\"]]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The amount of data was too large and the results were not available after one day.\n",
    "# We trained with a smaller amount of data.\n",
    "df = df.iloc[:5000, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(sentence):\n",
    "\n",
    "    sentence = ''.join([s.lower() for s in sentence if s not in string.punctuation])\n",
    "    sentence = ' '.join(stemmer.stem(w) for w in sentence.split())\n",
    "    tokens = nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(sentence)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[to, whom, did, the, virgin, mari, alleg, appe...</td>\n",
       "      <td>[saint, bernadett, soubir]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[what, is, in, front, of, the, notr, dame, mai...</td>\n",
       "      <td>[a, copper, statu, of, christ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[the, basilica, of, the, sacr, heart, at, notr...</td>\n",
       "      <td>[the, main, build]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[what, is, the, grotto, at, notr, dame]</td>\n",
       "      <td>[a, marian, place, of, prayer, and, reflect]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[what, sit, on, top, of, the, main, build, at,...</td>\n",
       "      <td>[a, golden, statu, of, the, virgin, mari]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  [to, whom, did, the, virgin, mari, alleg, appe...   \n",
       "1  [what, is, in, front, of, the, notr, dame, mai...   \n",
       "2  [the, basilica, of, the, sacr, heart, at, notr...   \n",
       "3            [what, is, the, grotto, at, notr, dame]   \n",
       "4  [what, sit, on, top, of, the, main, build, at,...   \n",
       "\n",
       "                                         Answer  \n",
       "0                    [saint, bernadett, soubir]  \n",
       "1                [a, copper, statu, of, christ]  \n",
       "2                            [the, main, build]  \n",
       "3  [a, marian, place, of, prayer, and, reflect]  \n",
       "4     [a, golden, statu, of, the, virgin, mari]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Question'] = df['Question'].apply(prepare_text)\n",
    "df['Answer'] = df['Answer'].apply(prepare_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPairs(df):\n",
    "\n",
    "    temp1 = df['Question'].apply(lambda x: \" \".join(x) ).to_list()\n",
    "    temp2 = df['Answer'].apply(lambda x: \" \".join(x) ).to_list()\n",
    "    \n",
    "    return [list(i) for i in zip(temp1, temp2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = getPairs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxLen(pairs):\n",
    "    \n",
    "    max_src = 0 \n",
    "    max_trg = 0\n",
    "    \n",
    "    for p in pairs:\n",
    "        max_src = len(p[0].split()) if len(p[0].split()) > max_src else max_src\n",
    "        max_trg = len(p[1].split()) if len(p[1].split()) > max_trg else max_trg\n",
    "        \n",
    "    return max_src, max_trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43, 29)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_src, max_trg = getMaxLen(pairs)\n",
    "max_trg, max_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self):\n",
    "        self.word2index = {\"\": SOS_token, \"\": EOS_token}\n",
    "        self.index2word = {SOS_token: \"\", EOS_token: \"\"}\n",
    "        self.words_count = len(self.word2index)\n",
    "\n",
    "    def add_words(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            if word not in self.word2index:\n",
    "                self.word2index[word] = self.words_count\n",
    "                self.index2word[self.words_count] = word\n",
    "                self.words_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Chatbot()\n",
    "TRG = Chatbot()\n",
    "\n",
    "for pair in pairs:\n",
    "    SRC.add_words(pair[0])\n",
    "    TRG.add_words(pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def toTensor(chatbot, sentence):\n",
    "\n",
    "    indices = [chatbot.word2index[word] for word in sentence.split(' ')]\n",
    "    indices.append(chatbot.word2index[''])\n",
    "    \n",
    "    return torch.Tensor(indices).long().to(device).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = [toTensor(SRC, pair[0]) for pair in pairs]\n",
    "target_data = [toTensor(TRG, pair[1]) for pair in pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Seq2Seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.input_size, self.hidden_size)\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell_state):\n",
    "        \n",
    "        x = self.embedding(x).view(1, 1, -1)\n",
    "        x, (hidden, cell_state) = self.lstm(x, (hidden, cell_state))\n",
    "        \n",
    "        return x, hidden, cell_state\n",
    "        \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, self.hidden_size)\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x, hidden, cell_state):\n",
    "        \n",
    "        x = self.embedding(x).view(1, 1, -1)\n",
    "        x, (hidden, cell_state) = self.lstm(x, (hidden, cell_state))\n",
    "        x = self.softmax(self.fc(x[0]))\n",
    "        \n",
    "        return x, hidden, cell_state\n",
    "    \n",
    "     \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.encoder = Encoder(self.input_size, self.hidden_size)\n",
    "        self.decoder = Decoder(self.hidden_size, self.output_size)\n",
    "        \n",
    "    def forward(self, src, trg, src_len, trg_len, teacher_force=1):\n",
    "        \n",
    "        output = {\n",
    "            'decoder_output':[]\n",
    "        }\n",
    "        \n",
    "        encoder_hidden = torch.zeros([1, 1, self.hidden_size]).to(device) \n",
    "        cell_state = torch.zeros([1, 1, self.hidden_size]).to(device)  \n",
    "        \n",
    "        for i in range(src_len):\n",
    "            encoder_output, encoder_hidden, cell_state = self.encoder(src[i], encoder_hidden, cell_state)\n",
    "\n",
    "        decoder_input = torch.Tensor([[0]]).long().to(device) \n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        for i in range(trg_len):\n",
    "            decoder_output, decoder_hidden, cell_state = self.decoder(decoder_input, decoder_hidden, cell_state)\n",
    "            output['decoder_output'].append(decoder_output)\n",
    "            \n",
    "            if self.training: \n",
    "                decoder_input = target_tensor[i] if random.random() > teacher_force else decoder_output.argmax(1) \n",
    "            else:\n",
    "                _, top_index = decoder_output.data.topk(1)\n",
    "                decoder_input = top_index.squeeze().detach()\n",
    "                \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(source_data, target_data, model, epochs, batch_size, print_every, learning_rate):\n",
    "    \n",
    "    model.to(device)\n",
    "    total_training_loss = 0\n",
    "    total_valid_loss = 0\n",
    "    loss = 0\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    kf = KFold(n_splits=epochs, shuffle=True)\n",
    "\n",
    "    for e, (train_index, test_index) in enumerate(kf.split(source_data), 1):\n",
    "        model.train()\n",
    "        for i in range(0, len(train_index)):\n",
    "\n",
    "            src = source_data[i]\n",
    "            trg = target_data[i]\n",
    "\n",
    "            output = model(src, trg, src.size(0), trg.size(0))\n",
    "\n",
    "            current_loss = 0\n",
    "            for (s, t) in zip(output[\"decoder_output\"], trg): \n",
    "                current_loss += criterion(s, t)\n",
    "\n",
    "            loss += current_loss\n",
    "            total_training_loss += (current_loss.item() / trg.size(0))\n",
    "\n",
    "            if i % batch_size == 0 or i == (len(train_index)-1):\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                loss = 0\n",
    "\n",
    "        model.eval()\n",
    "        for i in range(0, len(test_index)):\n",
    "            src = source_data[i]\n",
    "            trg = target_data[i]\n",
    "\n",
    "            output = model(src, trg, src.size(0), trg.size(0))\n",
    "\n",
    "            current_loss = 0\n",
    "            for (s, t) in zip(output[\"decoder_output\"], trg): \n",
    "                current_loss += criterion(s, t)\n",
    "\n",
    "            total_valid_loss += (current_loss.item() / trg.size(0))\n",
    "\n",
    "        if e % print_every == 0:\n",
    "            training_loss_average = total_training_loss / (len(train_index)*print_every)\n",
    "            validation_loss_average = total_valid_loss / (len(test_index)*print_every)\n",
    "            print(\"{}/{} Epoch  -  Training Loss = {:.4f}  -  Validation Loss = {:.4f}\".format(e, epochs, training_loss_average, validation_loss_average))\n",
    "            total_training_loss = 0\n",
    "            total_valid_loss = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/50 Epoch  -  Training Loss = 5.8499  -  Validation Loss = 5.8281\n",
      "10/50 Epoch  -  Training Loss = 5.3663  -  Validation Loss = 5.4062\n",
      "15/50 Epoch  -  Training Loss = 5.0204  -  Validation Loss = 5.0839\n",
      "20/50 Epoch  -  Training Loss = 4.5684  -  Validation Loss = 4.5774\n",
      "25/50 Epoch  -  Training Loss = 4.0227  -  Validation Loss = 4.0918\n",
      "30/50 Epoch  -  Training Loss = 3.3892  -  Validation Loss = 3.5543\n",
      "35/50 Epoch  -  Training Loss = 2.6756  -  Validation Loss = 2.9853\n",
      "40/50 Epoch  -  Training Loss = 2.0059  -  Validation Loss = 2.2922\n",
      "45/50 Epoch  -  Training Loss = 1.3721  -  Validation Loss = 1.6603\n",
      "50/50 Epoch  -  Training Loss = 0.8429  -  Validation Loss = 1.0303\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "hidden_size = 128\n",
    "batch_size = 128\n",
    "epochs = 50\n",
    "\n",
    "seq2seq = Seq2Seq(SRC.words_count, hidden_size, TRG.words_count)\n",
    "\n",
    "train(source_data = source_data,\n",
    "      target_data = target_data,\n",
    "      model = seq2seq,\n",
    "      epochs = epochs,\n",
    "      batch_size = batch_size,\n",
    "      print_every = 5,\n",
    "      learning_rate = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(src, SRC, TRG, model, target_max_len):\n",
    "    \n",
    "    try:\n",
    "        src = toTensor(SRC, \" \".join(prepare_text(src)))\n",
    "    except:\n",
    "        print(\"Error: I don't know!.\")\n",
    "        return\n",
    "    \n",
    "    answer_words = []\n",
    "    \n",
    "    output = model(src, None, src.size(0), target_max_len)\n",
    "\n",
    "    for tensor in output['decoder_output']:\n",
    "\n",
    "        _, top_token = tensor.data.topk(1)\n",
    "        if top_token.item() == 1:\n",
    "            break\n",
    "        else:\n",
    "            word = TRG.index2word[top_token.item()]\n",
    "            answer_words.append(word)\n",
    "            \n",
    "    print(\"<\", ' '.join(answer_words), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(4504, 128)\n",
       "    (lstm): LSTM(128, 128)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(4062, 128)\n",
       "    (lstm): LSTM(128, 128)\n",
       "    (fc): Linear(in_features=128, out_features=4062, bias=True)\n",
       "    (softmax): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(seq2seq, 'seq2seq.pt')\n",
    "\n",
    "seq2seq = torch.load('seq2seq.pt', map_location=torch.device('cuda'))\n",
    "seq2seq.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ex, Question & Answer List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">  To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? \n",
      "<  ['Saint Bernadette Soubirous'] \n",
      "\n",
      ">  What is in front of the Notre Dame Main Building? \n",
      "<  ['a copper statue of Christ'] \n",
      "\n",
      ">  The Basilica of the Sacred heart at Notre Dame is beside to which structure? \n",
      "<  ['the Main Building'] \n",
      "\n",
      ">  What is the Grotto at Notre Dame? \n",
      "<  ['a Marian place of prayer and reflection'] \n",
      "\n",
      ">  What sits on top of the Main Building at Notre Dame? \n",
      "<  ['a golden statue of the Virgin Mary'] \n",
      "\n",
      ">  When did the Scholastic Magazine of Notre dame begin publishing? \n",
      "<  ['September 1876'] \n",
      "\n",
      ">  How often is Notre Dame's the Juggler published? \n",
      "<  ['twice'] \n",
      "\n",
      ">  What is the daily student paper at Notre Dame called? \n",
      "<  ['The Observer'] \n",
      "\n",
      ">  How many student news papers are found at Notre Dame? \n",
      "<  ['three'] \n",
      "\n",
      ">  In what year did the student paper Common Sense begin publication at Notre Dame? \n",
      "<  ['1987'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = loadDF('.data')\n",
    "\n",
    "feature = [\"Sentence\", \"Question\", \"Answer\", \"?\"]\n",
    "df.columns = feature\n",
    "\n",
    "df = df[[\"Question\", \"Answer\"]]\n",
    "\n",
    "for i in range(0, 10): \n",
    "    print(\"> \", df.iloc[i,0], \"\\n< \", df.iloc[i,1], \"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 'exit' to finish the chat.\n",
      " -------------------------------------------------- \n",
      "\n",
      "> To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "<  \n",
      "\n",
      "> What is in front of the Notre Dame Main Building? \n",
      "< a copper of of a \n",
      "\n",
      "> The Basilica of the Sacred heart at Notre Dame is beside to which structure?\n",
      "< the main build \n",
      "\n",
      "> What is the Grotto at Notre Dame?\n",
      "< a marian place place in in \n",
      "\n",
      "> What sits on top of the Main Building at Notre Dame?\n",
      "< a golden statu of of of \n",
      "\n",
      "> exit\n"
     ]
    }
   ],
   "source": [
    "print(\"Type 'exit' to finish the chat.\\n\", \"-\"*50, '\\n')\n",
    "while (True):\n",
    "    src = input(\"> \")\n",
    "    if src.strip() == \"exit\":\n",
    "        break\n",
    "    evaluate(src, SRC, TRG, seq2seq, max_trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
